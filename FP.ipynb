{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious PLay vs Fictitious PLay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.mp import MP\n",
    "from agents.fictitiousplay import FictitiousPlay\n",
    "from agents.random_agent import RandomAgent\n",
    "from auxiliar.repeated_normalform_games_functions import iter_game, plot_rewards, plot_policies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_alias = {'agent_0':'FP-agent_0', 'agent_1':'FP-agent_1'}\n",
    "agent_classes = {'agent_0': FictitiousPlay, 'agent_1': FictitiousPlay}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Pennies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir juego\n",
    "g = MP()\n",
    "\n",
    "# Definir labels\n",
    "action_labels = ['$\\pi(H)$', '$\\pi(T)$']\n",
    "\n",
    "# Definir parámetros del experimento\n",
    "NITS = 5           # Cantidad de iteraciones\n",
    "NSTEPS = int(1e3)  # Cantidad de steps por iteración\n",
    "\n",
    "# Iterar juegos\n",
    "iter_game(NITS, NSTEPS, g, agent_classes, action_labels, agent_alias, plot_simplex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock, Paper, Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.rps import RPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir juego\n",
    "g = RPS()\n",
    "\n",
    "# Definir labels\n",
    "action_labels = ['$\\pi(R)$', '$\\pi(P)$', '$\\pi(S)$']\n",
    "\n",
    "# Definir parámetros del experimento\n",
    "NITS = 10           # Cantidad de iteraciones\n",
    "NSTEPS = int(10e3)  # Cantidad de steps por iteración\n",
    "\n",
    "# Iterar juegos\n",
    "rewards, policies = iter_game(NITS, NSTEPS, g, agent_classes, action_labels, agent_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_policy = {agent: agent_policies[:,:] for agent, agent_policies in policies.items()}\n",
    "N = len(iter_policy['agent_0'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(\n",
    "    iter_policy['agent_0'][:, 0],\n",
    "    iter_policy['agent_0'][:, 1],\n",
    "    label='Agent A',\n",
    "    color='tab:blue',\n",
    "    alpha=0.5\n",
    ")\n",
    "ax.plot(\n",
    "    1 - iter_policy['agent_1'][:, 0],\n",
    "    1 - iter_policy['agent_1'][:, 1],\n",
    "    label='Agent B',\n",
    "    color='orange',\n",
    "    alpha=0.5\n",
    ")\n",
    "ax.scatter(iter_policy['agent_0'][-1, 0], iter_policy['agent_0'][-1, 1],\n",
    "           color='tab:blue', marker='*', s=100, label='Converged policy agent A')\n",
    "\n",
    "ax.scatter(1 - iter_policy['agent_1'][-1, 0], 1 - iter_policy['agent_1'][-1, 1],\n",
    "           color='orange', marker='*', s=100, label='Converged policy agent B')\n",
    "ax.plot([0, 1], [1, 0], color='black', linestyle='--')\n",
    "\n",
    "ax.plot([iter_policy['agent_0'][-1, 0], iter_policy['agent_0'][-1, 1]], [iter_policy['agent_0'][-1, 0], 0], color='grey', linestyle='--')\n",
    "ax.plot([0, iter_policy['agent_0'][-1, 1]], [iter_policy['agent_0'][-1, 0], iter_policy['agent_0'][-1, 1]], color='grey', linestyle='--')\n",
    "\n",
    "ax.plot([1-iter_policy['agent_1'][-1, 0], 1-iter_policy['agent_1'][-1, 1]], [1-iter_policy['agent_1'][-1, 0], 1], color='grey', linestyle='--')\n",
    "ax.plot([1, 1-iter_policy['agent_1'][-1, 1]], [1-iter_policy['agent_1'][-1, 0], 1-iter_policy['agent_1'][-1, 1]], color='grey', linestyle='--')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel(\"$\\pi_{A}$(Rock)\", loc='left')\n",
    "ax.set_ylabel(\"$\\pi_{A}$(Paper)\", loc='bottom')\n",
    "\n",
    "ax_top = ax.secondary_xaxis('top', functions=(lambda x: 1 - x, lambda x: 1 - x))\n",
    "ax_top.set_xlabel(\"$\\pi_{B}$(Rock)\", loc='right')\n",
    "\n",
    "ax_right = ax.secondary_yaxis('right', functions=(lambda y: 1 - y, lambda y: 1 - y))\n",
    "ax_right.set_ylabel(\"$\\pi_{B}$(Paper)\", loc='top')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"Empirical Policy Trajectories\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blotto  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.blotto import Blotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir juego\n",
    "S = 10 # 10\n",
    "N = 4  # 5\n",
    "g = Blotto(S=S, N=N)\n",
    "\n",
    "# Definir labels\n",
    "action_labels = [f'$\\pi(a_{i}={g._moves[i]}$)' for i in range(g.action_spaces['agent_0'].n)]\n",
    "\n",
    "# Definir parámetros del experimento\n",
    "NITS = 10          # Cantidad de iteraciones\n",
    "NSTEPS = int(1e3) # Cantidad de steps por iteración\n",
    "\n",
    "# Iterar juegos\n",
    "iter_game(NITS, NSTEPS, g, agent_classes, action_labels, agent_alias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
